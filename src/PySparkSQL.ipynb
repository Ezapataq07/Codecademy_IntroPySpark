{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing PySpark SQL\n",
    "\n",
    "While we can directly analyze data using Spark’s Resilient Distributed Datasets (RDDs), we may not always want to perform complicated analysis directly on RDDs. Luckily, Spark offers a module called Spark SQL that can make common data analysis tasks simpler and faster. In this lesson, we’ll introduce Spark SQL and demonstrate how it can be a powerful tool for accelerating the analysis of distributed datasets.\n",
    "\n",
    "The name Spark SQL is an umbrella term, as there are several ways to interact with data when using this module. We’ll cover two of these methods using the PySpark API:\n",
    "\n",
    "* First, we’ll learn the basics of inspecting and querying data in a Spark DataFrame.\n",
    "\n",
    "* Then, we’ll perform these same operations using standard SQL directly in our PySpark code.\n",
    "\n",
    "Before using either method, we must start a SparkSession, the entry point to Spark SQL. The session is a wrapper around a sparkContext and contains all the metadata required to start working with distributed data.\n",
    "\n",
    "The code below uses SparkSession.builder to set configuration parameters and create a new session. In the following example, we set one configuration parameter (spark.app.name) and call the .getOrCreate() method to initialize the new SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the SparkContext for a session with SparkSession.sparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=learning_spark_sql>\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext) \n",
    "# <SparkContext master=local[*] appName=learning_spark_sql>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can use the SparkSession to create DataFrames, read external files, register tables, and run SQL queries over saved data. When we’re done with our analysis, we can clear the Spark cache and terminate the session with SparkSession.stop(). Now that we’re familiar with the basics of SparkSession, the next step is to begin using Spark SQL to interact with data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/configuration.html#available-properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark DataFrames\n",
    "\n",
    "A PySpark SQL DataFrame is a distributed collection of data with a specific row and column structure. Under the hood, DataFrames are built on top of RDDs. Like pandas, PySpark SQL DataFrames allow a developer to analyze data more easily than by writing functions directly on underlying data.\n",
    "\n",
    "DataFrames can be created manually from RDDs using rdd.toDF([\"names\", \"of\", \"columns\"]). In the example below, we create a DataFrame from a manually constructed RDD and name its columns article_title and view_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list\n",
    "hrly_views_rdd  = spark.sparkContext.parallelize([\n",
    "    [\"Betty_White\" , 288886],\n",
    "    [\"Main_Page\", 139564],\n",
    "    [\"New_Year's_Day\", 7892],\n",
    "    [\"ABBA\", 8154]\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "hrly_views_df = hrly_views_rdd\\\n",
    "    .toDF([\"article_title\", \"view_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at our new DataFrame. We can use the DataFrame.show(n_rows) method to print the first n_rows of a Spark DataFrame. It can also be helpful to pass truncate=False to ensure all columns are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|article_title |view_count|\n",
      "+--------------+----------+\n",
      "|Betty_White   |288886    |\n",
      "|Main_Page     |139564    |\n",
      "|New_Year's_Day|7892      |\n",
      "|ABBA          |8154      |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that this data is loaded in as a DataFrame, we can access the underlying RDD with DataFrame.rdd. You likely won’t need the underlying data often, but it can be helpful to keep in mind that a DataFrame is a structure built on top of an RDD. When we check the type of hrly_views_df_rdd, we can see that it’s an RDD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Access DataFrame's underlying RDD\n",
    "hrly_views_df_rdd = hrly_views_df.rdd\n",
    "\n",
    "# Check object type\n",
    "print(type(hrly_views_df_rdd)) \n",
    "# <class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Because we learned about SparkSession in the first exercise, all remaining exercises in this lesson will include the code to create a SparkSession named spark for you to use. Be sure to run these cells!\n",
    "\n",
    "Using the RDD sample_page_views, create a DataFrame named sample_page_views_df with columns named language_code, title, date, and count.\n",
    "\n",
    "In the same code cell, add code to show the first five rows of the DataFrame. Set truncate=False to ensure all columns are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sample_page_views  = spark.sparkContext.parallelize([\n",
    "    [\"en\", \"Statue_of_Liberty\", \"2022-01-01\", 263],\n",
    "    [\"en\", \"Replicas_of_the_Statue_of_Liberty\", \"2022-01-01\", 11],\n",
    "    [\"en\", \"Statue_of_Lucille_Ball\" ,\"2022-01-01\", 6],\n",
    "    [\"en\", \"Statue_of_Liberty_National_Monument\", \"2022-01-01\", 4],\n",
    "    [\"en\", \"Statue_of_Liberty_play\"  ,\"2022-01-01\", 3],  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------+----------+-----+\n",
      "|language_code|title                              |date      |count|\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "|en           |Statue_of_Liberty                  |2022-01-01|263  |\n",
      "|en           |Replicas_of_the_Statue_of_Liberty  |2022-01-01|11   |\n",
      "|en           |Statue_of_Lucille_Ball             |2022-01-01|6    |\n",
      "|en           |Statue_of_Liberty_National_Monument|2022-01-01|4    |\n",
      "|en           |Statue_of_Liberty_play             |2022-01-01|3    |\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sample_page_views_df = sample_page_views.toDF(['language_code', 'title', 'date', 'count'])\n",
    "\n",
    "# show first 5 rows\n",
    "sample_page_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Access the RDD underlying sample_page_views_df and save it as sample_page_views_rdd_restored. In the same code cell, run sample_page_views_rdd_restored.collect() to view the restored RDD.\n",
    "\n",
    "Note: You may notice that the restored RDD is not identical to the original RDD! Although the data is the same, when we converted the data to a DataFrame, PySpark automatically wrapped the original content into a Row. Behind the scenes, rows allow for more efficient calculations over large distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language_code='en', title='Statue_of_Liberty', date='2022-01-01', count=263),\n",
       " Row(language_code='en', title='Replicas_of_the_Statue_of_Liberty', date='2022-01-01', count=11),\n",
       " Row(language_code='en', title='Statue_of_Lucille_Ball', date='2022-01-01', count=6),\n",
       " Row(language_code='en', title='Statue_of_Liberty_National_Monument', date='2022-01-01', count=4),\n",
       " Row(language_code='en', title='Statue_of_Liberty_play', date='2022-01-01', count=3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sample_page_views_rdd_restored = sample_page_views_df.rdd\n",
    "\n",
    "# show restored RDD\n",
    "sample_page_views_rdd_restored.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames from External Sources\n",
    "\n",
    "In this exercise, we’ll learn how to pull in larger datasets from external sources. To start, we’ll be using a dataset from Wikipedia that counts views of all articles by hour. For demonstration’s sake, we’ll use the first hour of 2022. Let’s take a look at the code we might use to read a CSV of this data from a location on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.readwriter.DataFrameReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(spark.read)) \n",
    "# <class 'pyspark.sql.readwriter.DataFrameReader'>\n",
    "\n",
    "# Read CSV to DataFrame\n",
    "hrly_views_df = spark.read.option('header', True).option('delimiter', ' ').option('inferSchema', True).csv('views_2022_01_01_000000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things going on in this code, let’s go through them one at a time:\n",
    "\n",
    "This code uses the SparkSession.read function to create a new DataFrameReader\n",
    "\n",
    "The DataFrameReader has an .option('option_name', 'option_value') method that can be used to instruct Spark how exactly to read a file. In this case, we used the following options:\n",
    "\n",
    "* .option('header', True) — Indicate the file already contains a header row. By default, Spark assumes there is no header.\n",
    "\n",
    "* .option('delimiter', ' ') — Indicates each column is separated by a space (‘ ‘). By default, Spark assumes CSV columns are separated by commas.\n",
    "\n",
    "* .option('inferSchema', True) — Instructs Spark to sample a subset of rows before determining each column’s type. By default, Spark will treat all CSV columns as strings.\n",
    "\n",
    "The DataFrameReader also has a .csv('path') method which loads a CSV file and returns the result as a DataFrame. There are a few quick ways of checking that our data has been read in properly. The most direct way is checking DataFrame.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display first 5 rows of DataFrame\n",
    "hrly_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks Good! In this exercise, we used a DataFrameReader to pull a CSV from disk into our local Spark environment. However, Spark can read a wide variety of file formats. You can refer to the PySpark documentation to explore all available DataFrameReader options and file formats. In the following exercise, we’ll start to analyze the contents of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/search.html?q=DataFrameReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file wiki_uniq_march_2022.csv contains the estimated count of unique visitors to each Wikipedia domain on March 1st, 2022. The file has the following layout:\n",
    "\n",
    "* Site/Project Name (string)\n",
    "\n",
    "* Estimated Human Visitors (int)\n",
    "\n",
    "* Estimated Bot Visitors (int)\n",
    "\n",
    "* Total Traffic (int)\n",
    "\n",
    "You can read more about how Wikipedia estimates these values here.\n",
    "\n",
    "First, let’s load the data from wiki_uniq_march_2022.csv as a DataFrame named wiki_uniq_df and display the first 10 rows in the notebook with truncate = False. For the moment, do not add any options when reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|_c0               |_c1                |_c2              |_c3                |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "|de.m.wikipedia.org|4439596            |853251           |5292847            |\n",
      "|fr.m.wikipedia.org|3798528            |904567           |4703095            |\n",
      "|ru.wikipedia.org  |2852296            |687501           |3539797            |\n",
      "|es.wikipedia.org  |2460489            |962516           |3423005            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_df = spark.read.csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the first 10 rows\n",
    "wiki_uniq_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve read the file, but the result doesn’t quite look right! This file has a header row.\n",
    "\n",
    "Pass the option to the DataFrameReader that will read the file and create a header from the first row. Name this DataFrame wiki_uniq_w_header_df. In the same code cell, show the first 10 rows of wiki_uniq_w_header_df with truncate=False to check the header has loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "|de.m.wikipedia.org|4439596            |853251           |5292847            |\n",
      "|fr.m.wikipedia.org|3798528            |904567           |4703095            |\n",
      "|ru.wikipedia.org  |2852296            |687501           |3539797            |\n",
      "|es.wikipedia.org  |2460489            |962516           |3423005            |\n",
      "|it.m.wikipedia.org|2806943            |566876           |3373819            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_w_header_df = spark.read\\\n",
    "    .option('header', True)\\\n",
    "    .csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the first 10 rows\n",
    "wiki_uniq_w_header_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domain', 'string'),\n",
       " ('uniq_human_visitors', 'string'),\n",
       " ('uniq_bot_visitors', 'string'),\n",
       " ('total_visitor_count', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the data types\n",
    "wiki_uniq_w_header_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is better, but we haven’t specified the types for the DataFrame yet. Check the data types for each column in wiki_uniq_w_header_df by running the provided code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like PySpark defaults to strings as the data types for all columns when we don’t specify them somehow.\n",
    "\n",
    "Read the data in again, this time passing an option to the DataFrameReader that will tell Spark to sample rows to determine the file schema. Name this DataFrame wiki_uniq_w_schema_df. In the same code cell, run the provided code to check the data types in the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domain', 'string'),\n",
       " ('uniq_human_visitors', 'int'),\n",
       " ('uniq_bot_visitors', 'int'),\n",
       " ('total_visitor_count', 'int')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_w_schema_df = spark.read\\\n",
    "    .option('header', True)\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the data types\n",
    "wiki_uniq_w_schema_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting and Cleaning Data With PySpark\n",
    "\n",
    "In this exercise, we’re going to start to analyze our pageview data and learn how Spark can help with data exploration. Like Pandas, Spark DataFrames offer a series of operations for cleaning, inspecting, and transforming data. Earlier in the lesson, we mentioned that all DataFrames have a schema that defines their structure, columns, and datatypes. We can use DataFrame.printSchema() to show a DataFrame’s schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domain: string (nullable = true)\n",
      " |-- uniq_human_visitors: integer (nullable = true)\n",
      " |-- uniq_bot_visitors: integer (nullable = true)\n",
      " |-- total_visitor_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame schema\n",
    "hrly_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use DataFrame.describe() to see a high-level summary of the data by column. The result of DataFrame.describe() is a DataFrame in itself, so we append .show() to get it to display in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|summary|domain          |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|count  |760             |760                |760              |760                |\n",
      "|mean   |NULL            |155413.0394736842  |51431.0552631579 |206844.09473684212 |\n",
      "|stddev |NULL            |1435327.5409314982 |376318.441663093 |1809320.9789242456 |\n",
      "|min    |aa.wikibooks.org|0                  |170              |1005               |\n",
      "|max    |zu.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df_desc = hrly_views_df.describe()\n",
    "hrly_views_df_desc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data was taken from the first hour of the month, it looks like the column monthly_count only contains zeros. Because it contains no meaningful information, we can drop this field with DataFrame.drop(\"columns\", \"to\", \"drop\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is starting to look pretty good, but let’s make one more adjustment. The column article_title is a bit misleading: it seems this data contains articles, files, image pages, and wikipedia metadata pages. We can replace this misleading header with a better name using DataFrame.withColumnRenamed()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that Spark assigned all columns nullable = true. Intuitively, we know that article_title shouldn’t be null, but when the DataFrameReader reads a CSV, it assigns nullable = true to all columns. This is fine for now, but in some scenarios, you may wish to explicitly define a file’s schema. If interested, you can refer to PySpark’s documentation on defining a file’s schema. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.schema.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to read in the Wikipedia unique data has already been written. Let’s find out if the number and types of columns in the DataFrame look correct. Print the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read in the Wikipedia unique visitors dataset\n",
    "uniq_views_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domain: string (nullable = true)\n",
      " |-- uniq_human_visitors: integer (nullable = true)\n",
      " |-- uniq_bot_visitors: integer (nullable = true)\n",
      " |-- total_visitor_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s summarize this data and find out the mean total number of visitors per site. Save a high-level summary of the DataFrame to a new DataFrame named uniq_counts_df_desc and display it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|summary|          domain|uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|  count|             760|                760|              760|                760|\n",
      "|   mean|            NULL|  155413.0394736842| 51431.0552631579| 206844.09473684212|\n",
      "| stddev|            NULL| 1435327.5409314982| 376318.441663093| 1809320.9789242456|\n",
      "|    min|aa.wikibooks.org|                  0|              170|               1005|\n",
      "|    max|zu.wikipedia.org|           33261399|          8400247|           41661646|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_views_df_desc = uniq_views_df.describe()\n",
    "\n",
    "# show summary\n",
    "uniq_views_df_desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that the mean total number of visitors per site is about 206,844. Let’s assume our analysis is focused on only the uniq_human_visitors. Write code to drop total_visitor_count and uniq_bot_visitors. Save the result to a DataFrame named uniq_counts_human_df and show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|            domain|uniq_human_visitors|\n",
      "+------------------+-------------------+\n",
      "|en.m.wikipedia.org|           33261399|\n",
      "|  en.wikipedia.org|           17009339|\n",
      "|es.m.wikipedia.org|            5668575|\n",
      "|ru.m.wikipedia.org|            5816762|\n",
      "|ja.m.wikipedia.org|            5396108|\n",
      "+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_counts_human_df = uniq_views_df.drop('uniq_bot_visitors','total_visitor_count')\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_human_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s rename the column uniq_human_visitors to something a bit more descriptive. Rename uniq_human_visitors to unique_site_visitors. Save the new DataFrame as uniq_counts_final_df and show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|            domain|unique_site_visitors|\n",
      "+------------------+--------------------+\n",
      "|en.m.wikipedia.org|            33261399|\n",
      "|  en.wikipedia.org|            17009339|\n",
      "|es.m.wikipedia.org|             5668575|\n",
      "|ru.m.wikipedia.org|             5816762|\n",
      "|ja.m.wikipedia.org|             5396108|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_counts_final_df = uniq_counts_human_df.withColumnRenamed('uniq_human_visitors', 'unique_site_visitors' )\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataPySpark.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
