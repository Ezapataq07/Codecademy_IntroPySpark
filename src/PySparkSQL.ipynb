{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing PySpark SQL\n",
    "\n",
    "While we can directly analyze data using Spark’s Resilient Distributed Datasets (RDDs), we may not always want to perform complicated analysis directly on RDDs. Luckily, Spark offers a module called Spark SQL that can make common data analysis tasks simpler and faster. In this lesson, we’ll introduce Spark SQL and demonstrate how it can be a powerful tool for accelerating the analysis of distributed datasets.\n",
    "\n",
    "The name Spark SQL is an umbrella term, as there are several ways to interact with data when using this module. We’ll cover two of these methods using the PySpark API:\n",
    "\n",
    "* First, we’ll learn the basics of inspecting and querying data in a Spark DataFrame.\n",
    "\n",
    "* Then, we’ll perform these same operations using standard SQL directly in our PySpark code.\n",
    "\n",
    "Before using either method, we must start a SparkSession, the entry point to Spark SQL. The session is a wrapper around a sparkContext and contains all the metadata required to start working with distributed data.\n",
    "\n",
    "The code below uses SparkSession.builder to set configuration parameters and create a new session. In the following example, we set one configuration parameter (spark.app.name) and call the .getOrCreate() method to initialize the new SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the SparkContext for a session with SparkSession.sparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=learning_spark_sql>\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext) \n",
    "# <SparkContext master=local[*] appName=learning_spark_sql>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can use the SparkSession to create DataFrames, read external files, register tables, and run SQL queries over saved data. When we’re done with our analysis, we can clear the Spark cache and terminate the session with SparkSession.stop(). Now that we’re familiar with the basics of SparkSession, the next step is to begin using Spark SQL to interact with data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/configuration.html#available-properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark DataFrames\n",
    "\n",
    "A PySpark SQL DataFrame is a distributed collection of data with a specific row and column structure. Under the hood, DataFrames are built on top of RDDs. Like pandas, PySpark SQL DataFrames allow a developer to analyze data more easily than by writing functions directly on underlying data.\n",
    "\n",
    "DataFrames can be created manually from RDDs using rdd.toDF([\"names\", \"of\", \"columns\"]). In the example below, we create a DataFrame from a manually constructed RDD and name its columns article_title and view_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list\n",
    "hrly_views_rdd  = spark.sparkContext.parallelize([\n",
    "    [\"Betty_White\" , 288886],\n",
    "    [\"Main_Page\", 139564],\n",
    "    [\"New_Year's_Day\", 7892],\n",
    "    [\"ABBA\", 8154]\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "hrly_views_df = hrly_views_rdd\\\n",
    "    .toDF([\"article_title\", \"view_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at our new DataFrame. We can use the DataFrame.show(n_rows) method to print the first n_rows of a Spark DataFrame. It can also be helpful to pass truncate=False to ensure all columns are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|article_title |view_count|\n",
      "+--------------+----------+\n",
      "|Betty_White   |288886    |\n",
      "|Main_Page     |139564    |\n",
      "|New_Year's_Day|7892      |\n",
      "|ABBA          |8154      |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that this data is loaded in as a DataFrame, we can access the underlying RDD with DataFrame.rdd. You likely won’t need the underlying data often, but it can be helpful to keep in mind that a DataFrame is a structure built on top of an RDD. When we check the type of hrly_views_df_rdd, we can see that it’s an RDD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Access DataFrame's underlying RDD\n",
    "hrly_views_df_rdd = hrly_views_df.rdd\n",
    "\n",
    "# Check object type\n",
    "print(type(hrly_views_df_rdd)) \n",
    "# <class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Because we learned about SparkSession in the first exercise, all remaining exercises in this lesson will include the code to create a SparkSession named spark for you to use. Be sure to run these cells!\n",
    "\n",
    "Using the RDD sample_page_views, create a DataFrame named sample_page_views_df with columns named language_code, title, date, and count.\n",
    "\n",
    "In the same code cell, add code to show the first five rows of the DataFrame. Set truncate=False to ensure all columns are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sample_page_views  = spark.sparkContext.parallelize([\n",
    "    [\"en\", \"Statue_of_Liberty\", \"2022-01-01\", 263],\n",
    "    [\"en\", \"Replicas_of_the_Statue_of_Liberty\", \"2022-01-01\", 11],\n",
    "    [\"en\", \"Statue_of_Lucille_Ball\" ,\"2022-01-01\", 6],\n",
    "    [\"en\", \"Statue_of_Liberty_National_Monument\", \"2022-01-01\", 4],\n",
    "    [\"en\", \"Statue_of_Liberty_play\"  ,\"2022-01-01\", 3],  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------+----------+-----+\n",
      "|language_code|title                              |date      |count|\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "|en           |Statue_of_Liberty                  |2022-01-01|263  |\n",
      "|en           |Replicas_of_the_Statue_of_Liberty  |2022-01-01|11   |\n",
      "|en           |Statue_of_Lucille_Ball             |2022-01-01|6    |\n",
      "|en           |Statue_of_Liberty_National_Monument|2022-01-01|4    |\n",
      "|en           |Statue_of_Liberty_play             |2022-01-01|3    |\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sample_page_views_df = sample_page_views.toDF(['language_code', 'title', 'date', 'count'])\n",
    "\n",
    "# show first 5 rows\n",
    "sample_page_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Access the RDD underlying sample_page_views_df and save it as sample_page_views_rdd_restored. In the same code cell, run sample_page_views_rdd_restored.collect() to view the restored RDD.\n",
    "\n",
    "Note: You may notice that the restored RDD is not identical to the original RDD! Although the data is the same, when we converted the data to a DataFrame, PySpark automatically wrapped the original content into a Row. Behind the scenes, rows allow for more efficient calculations over large distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language_code='en', title='Statue_of_Liberty', date='2022-01-01', count=263),\n",
       " Row(language_code='en', title='Replicas_of_the_Statue_of_Liberty', date='2022-01-01', count=11),\n",
       " Row(language_code='en', title='Statue_of_Lucille_Ball', date='2022-01-01', count=6),\n",
       " Row(language_code='en', title='Statue_of_Liberty_National_Monument', date='2022-01-01', count=4),\n",
       " Row(language_code='en', title='Statue_of_Liberty_play', date='2022-01-01', count=3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "sample_page_views_rdd_restored = sample_page_views_df.rdd\n",
    "\n",
    "# show restored RDD\n",
    "sample_page_views_rdd_restored.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames from External Sources\n",
    "\n",
    "In this exercise, we’ll learn how to pull in larger datasets from external sources. To start, we’ll be using a dataset from Wikipedia that counts views of all articles by hour. For demonstration’s sake, we’ll use the first hour of 2022. Let’s take a look at the code we might use to read a CSV of this data from a location on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.readwriter.DataFrameReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(spark.read)) \n",
    "# <class 'pyspark.sql.readwriter.DataFrameReader'>\n",
    "\n",
    "# Read CSV to DataFrame\n",
    "hrly_views_df = spark.read.option('header', True).option('delimiter', ' ').option('inferSchema', True).csv('views_2022_01_01_000000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things going on in this code, let’s go through them one at a time:\n",
    "\n",
    "This code uses the SparkSession.read function to create a new DataFrameReader\n",
    "\n",
    "The DataFrameReader has an .option('option_name', 'option_value') method that can be used to instruct Spark how exactly to read a file. In this case, we used the following options:\n",
    "\n",
    "* .option('header', True) — Indicate the file already contains a header row. By default, Spark assumes there is no header.\n",
    "\n",
    "* .option('delimiter', ' ') — Indicates each column is separated by a space (‘ ‘). By default, Spark assumes CSV columns are separated by commas.\n",
    "\n",
    "* .option('inferSchema', True) — Instructs Spark to sample a subset of rows before determining each column’s type. By default, Spark will treat all CSV columns as strings.\n",
    "\n",
    "The DataFrameReader also has a .csv('path') method which loads a CSV file and returns the result as a DataFrame. There are a few quick ways of checking that our data has been read in properly. The most direct way is checking DataFrame.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display first 5 rows of DataFrame\n",
    "hrly_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks Good! In this exercise, we used a DataFrameReader to pull a CSV from disk into our local Spark environment. However, Spark can read a wide variety of file formats. You can refer to the PySpark documentation to explore all available DataFrameReader options and file formats. In the following exercise, we’ll start to analyze the contents of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/search.html?q=DataFrameReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file wiki_uniq_march_2022.csv contains the estimated count of unique visitors to each Wikipedia domain on March 1st, 2022. The file has the following layout:\n",
    "\n",
    "* Site/Project Name (string)\n",
    "\n",
    "* Estimated Human Visitors (int)\n",
    "\n",
    "* Estimated Bot Visitors (int)\n",
    "\n",
    "* Total Traffic (int)\n",
    "\n",
    "You can read more about how Wikipedia estimates these values here.\n",
    "\n",
    "First, let’s load the data from wiki_uniq_march_2022.csv as a DataFrame named wiki_uniq_df and display the first 10 rows in the notebook with truncate = False. For the moment, do not add any options when reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|_c0               |_c1                |_c2              |_c3                |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "|de.m.wikipedia.org|4439596            |853251           |5292847            |\n",
      "|fr.m.wikipedia.org|3798528            |904567           |4703095            |\n",
      "|ru.wikipedia.org  |2852296            |687501           |3539797            |\n",
      "|es.wikipedia.org  |2460489            |962516           |3423005            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_df = spark.read.csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the first 10 rows\n",
    "wiki_uniq_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve read the file, but the result doesn’t quite look right! This file has a header row.\n",
    "\n",
    "Pass the option to the DataFrameReader that will read the file and create a header from the first row. Name this DataFrame wiki_uniq_w_header_df. In the same code cell, show the first 10 rows of wiki_uniq_w_header_df with truncate=False to check the header has loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|domain            |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "|en.m.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "|en.wikipedia.org  |17009339           |4851741          |21861080           |\n",
      "|es.m.wikipedia.org|5668575            |1977289          |7645864            |\n",
      "|ru.m.wikipedia.org|5816762            |1367179          |7183941            |\n",
      "|ja.m.wikipedia.org|5396108            |1325212          |6721320            |\n",
      "|de.m.wikipedia.org|4439596            |853251           |5292847            |\n",
      "|fr.m.wikipedia.org|3798528            |904567           |4703095            |\n",
      "|ru.wikipedia.org  |2852296            |687501           |3539797            |\n",
      "|es.wikipedia.org  |2460489            |962516           |3423005            |\n",
      "|it.m.wikipedia.org|2806943            |566876           |3373819            |\n",
      "+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_w_header_df = spark.read\\\n",
    "    .option('header', True)\\\n",
    "    .csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the first 10 rows\n",
    "wiki_uniq_w_header_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domain', 'string'),\n",
       " ('uniq_human_visitors', 'string'),\n",
       " ('uniq_bot_visitors', 'string'),\n",
       " ('total_visitor_count', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the data types\n",
    "wiki_uniq_w_header_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is better, but we haven’t specified the types for the DataFrame yet. Check the data types for each column in wiki_uniq_w_header_df by running the provided code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like PySpark defaults to strings as the data types for all columns when we don’t specify them somehow.\n",
    "\n",
    "Read the data in again, this time passing an option to the DataFrameReader that will tell Spark to sample rows to determine the file schema. Name this DataFrame wiki_uniq_w_schema_df. In the same code cell, run the provided code to check the data types in the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domain', 'string'),\n",
       " ('uniq_human_visitors', 'int'),\n",
       " ('uniq_bot_visitors', 'int'),\n",
       " ('total_visitor_count', 'int')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "wiki_uniq_w_schema_df = spark.read\\\n",
    "    .option('header', True)\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the data types\n",
    "wiki_uniq_w_schema_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting and Cleaning Data With PySpark\n",
    "\n",
    "In this exercise, we’re going to start to analyze our pageview data and learn how Spark can help with data exploration. Like Pandas, Spark DataFrames offer a series of operations for cleaning, inspecting, and transforming data. Earlier in the lesson, we mentioned that all DataFrames have a schema that defines their structure, columns, and datatypes. We can use DataFrame.printSchema() to show a DataFrame’s schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domain: string (nullable = true)\n",
      " |-- uniq_human_visitors: integer (nullable = true)\n",
      " |-- uniq_bot_visitors: integer (nullable = true)\n",
      " |-- total_visitor_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame schema\n",
    "hrly_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use DataFrame.describe() to see a high-level summary of the data by column. The result of DataFrame.describe() is a DataFrame in itself, so we append .show() to get it to display in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|summary|domain          |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|count  |760             |760                |760              |760                |\n",
      "|mean   |NULL            |155413.0394736842  |51431.0552631579 |206844.09473684212 |\n",
      "|stddev |NULL            |1435327.5409314982 |376318.441663093 |1809320.9789242456 |\n",
      "|min    |aa.wikibooks.org|0                  |170              |1005               |\n",
      "|max    |zu.wikipedia.org|33261399           |8400247          |41661646           |\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df_desc = hrly_views_df.describe()\n",
    "hrly_views_df_desc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data was taken from the first hour of the month, it looks like the column monthly_count only contains zeros. Because it contains no meaningful information, we can drop this field with DataFrame.drop(\"columns\", \"to\", \"drop\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is starting to look pretty good, but let’s make one more adjustment. The column article_title is a bit misleading: it seems this data contains articles, files, image pages, and wikipedia metadata pages. We can replace this misleading header with a better name using DataFrame.withColumnRenamed()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that Spark assigned all columns nullable = true. Intuitively, we know that article_title shouldn’t be null, but when the DataFrameReader reads a CSV, it assigns nullable = true to all columns. This is fine for now, but in some scenarios, you may wish to explicitly define a file’s schema. If interested, you can refer to PySpark’s documentation on defining a file’s schema. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.schema.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to read in the Wikipedia unique data has already been written. Let’s find out if the number and types of columns in the DataFrame look correct. Print the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read in the Wikipedia unique visitors dataset\n",
    "uniq_views_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domain: string (nullable = true)\n",
      " |-- uniq_human_visitors: integer (nullable = true)\n",
      " |-- uniq_bot_visitors: integer (nullable = true)\n",
      " |-- total_visitor_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s summarize this data and find out the mean total number of visitors per site. Save a high-level summary of the DataFrame to a new DataFrame named uniq_counts_df_desc and display it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|summary|          domain|uniq_human_visitors|uniq_bot_visitors|total_visitor_count|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "|  count|             760|                760|              760|                760|\n",
      "|   mean|            NULL|  155413.0394736842| 51431.0552631579| 206844.09473684212|\n",
      "| stddev|            NULL| 1435327.5409314982| 376318.441663093| 1809320.9789242456|\n",
      "|    min|aa.wikibooks.org|                  0|              170|               1005|\n",
      "|    max|zu.wikipedia.org|           33261399|          8400247|           41661646|\n",
      "+-------+----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_views_df_desc = uniq_views_df.describe()\n",
    "\n",
    "# show summary\n",
    "uniq_views_df_desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that the mean total number of visitors per site is about 206,844. Let’s assume our analysis is focused on only the uniq_human_visitors. Write code to drop total_visitor_count and uniq_bot_visitors. Save the result to a DataFrame named uniq_counts_human_df and show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|            domain|uniq_human_visitors|\n",
      "+------------------+-------------------+\n",
      "|en.m.wikipedia.org|           33261399|\n",
      "|  en.wikipedia.org|           17009339|\n",
      "|es.m.wikipedia.org|            5668575|\n",
      "|ru.m.wikipedia.org|            5816762|\n",
      "|ja.m.wikipedia.org|            5396108|\n",
      "+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_counts_human_df = uniq_views_df.drop('uniq_bot_visitors','total_visitor_count')\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_human_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s rename the column uniq_human_visitors to something a bit more descriptive. Rename uniq_human_visitors to unique_site_visitors. Save the new DataFrame as uniq_counts_final_df and show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|            domain|unique_site_visitors|\n",
      "+------------------+--------------------+\n",
      "|en.m.wikipedia.org|            33261399|\n",
      "|  en.wikipedia.org|            17009339|\n",
      "|es.m.wikipedia.org|             5668575|\n",
      "|ru.m.wikipedia.org|             5816762|\n",
      "|ja.m.wikipedia.org|             5396108|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "uniq_counts_final_df = uniq_counts_human_df.withColumnRenamed('uniq_human_visitors', 'unique_site_visitors' )\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying PySpark DataFrames\n",
    "\n",
    "It’s time to start performing some analysis–this is where PySpark SQL really shines. PySpark SQL DataFrames have a variety of built-in methods that can help with analyzing data. Let’s get into a few examples!\n",
    "\n",
    "Imagine we’d like to filter our data to pages from a specific Wikipedia language_code (e.g., \"kw.m\"). This site is not very active, so it’s easy to use all of this hour’s data for demonstration purposes. We can display this result with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------+-------------------+-------------+----------+\n",
      "|domain             |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|language_code|site_type |\n",
      "+-------------------+-------------------+-----------------+-------------------+-------------+----------+\n",
      "|hi.m.wikipedia.org |372427             |197143           |569570             |hi           |wikipedia |\n",
      "|hi.wikipedia.org   |14706              |7714             |22420              |hi           |wikipedia |\n",
      "|hi.m.wiktionary.org|718                |3446             |4164               |hi           |wiktionary|\n",
      "|hi.m.wikibooks.org |720                |1661             |2381               |hi           |wikibooks |\n",
      "|hi.wiktionary.org  |116                |940              |1056               |hi           |wiktionary|\n",
      "+-------------------+-------------------+-----------------+-------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")\n",
    "\n",
    "hrly_views_df\\\n",
    "    .filter(hrly_views_df.language_code == \"hi\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the DataFrame.filter() method to select relevant rows. This is analogous to a SQL “WHERE” clause. In this case, our condition checks the column language_code for the value \"kw.m\". What if we want to remove the monthly_count column and display the data ordered by the hourly_count? To do so, we could use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `article_title` cannot be resolved. Did you mean one of the following? [`site_type`, `domain`, `language_code`, `uniq_bot_visitors`, `uniq_human_visitors`].;\n'Project [language_code#1037, 'article_title, 'hourly_count]\n+- Filter (language_code#1037 = kw.m)\n   +- Relation [domain#1033,uniq_human_visitors#1034,uniq_bot_visitors#1035,total_visitor_count#1036,language_code#1037,site_type#1038] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[43mhrly_views_df\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhrly_views_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkw.m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhourly_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhourly_count\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3186\u001b[0m \n\u001b[0;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `article_title` cannot be resolved. Did you mean one of the following? [`site_type`, `domain`, `language_code`, `uniq_bot_visitors`, `uniq_human_visitors`].;\n'Project [language_code#1037, 'article_title, 'hourly_count]\n+- Filter (language_code#1037 = kw.m)\n   +- Relation [domain#1033,uniq_human_visitors#1034,uniq_bot_visitors#1035,total_visitor_count#1036,language_code#1037,site_type#1038] csv\n"
     ]
    }
   ],
   "source": [
    "hrly_views_df\\\n",
    "    .filter(hrly_views_df.language_code == \"kw.m\")\\\n",
    "    .select(['language_code', 'article_title', 'hourly_count'])\\\n",
    "    .orderBy('hourly_count', ascending=False).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrame.select() is used to choose which columns to return in our result. You can think of DataFrame.select([\"A\", \"B\", \"C\"]) as analogous to SELECT A, B, C FROM DataFrame in SQL.\n",
    "\n",
    "* DataFrame.orderBy() is analogous to SQL’s ORDER BY. We use .orderBy('hourly_count', ascending=False) to specify the sort column and order logic. This would be analogous to ORDER BY hourly_count DESC in SQL.\n",
    "\n",
    "What if we’d like to select the sum of hourly_count by language_code? This could help us answer questions like “Which sites were most active this hour?” We can do that with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .select(['language_code', 'hourly_count'])\\\n",
    "    .groupBy('language_code')\\\n",
    "    .sum() \\\n",
    "    .orderBy('sum(hourly_count)', ascending=False)\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses DataFrame.groupBy('language_code').sum() to calculate the sum of all columns grouped by language_code, .groupBy(field) and .sum() are analogous to SQL’s GROUP BY and SUM functions respectively. This code also orders our results with .orderBy(), using the name of the constructed column, 'sum(hourly_count)'.\n",
    "\n",
    "There are many ways to use the DataFrame methods to query our data. However, if you’re familiar with SQL, you may prefer to use standard SQL statements. In the next section, we’ll explore how you can use standard SQL to explore data with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, a slightly modified version of the dataset you’ve been working with has been created (‘wiki_uniq_march_2022_w_site_type.csv’). The dataset now has two additional columns: language_code and site_type. Let’s find out how many domains there are for the \"ar\" language code. Filter the dataset to the rows for the language code \"ar\". Save your result as a DataFrame named ar_site_visitors and display it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Create a New SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "|              domain|uniq_human_visitors|uniq_bot_visitors|total_visitor_count|language_code|  site_type|\n",
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "|  ar.m.wikipedia.org|            1644253|           750620|            2394873|           ar|  wikipedia|\n",
      "|    ar.wikipedia.org|             212695|            97700|             310395|           ar|  wikipedia|\n",
      "| ar.m.wikisource.org|              56124|            52885|             109009|           ar| wikisource|\n",
      "|   ar.wikisource.org|               2134|             4355|               6489|           ar| wikisource|\n",
      "|  ar.m.wikiquote.org|                776|             3511|               4287|           ar|  wikiquote|\n",
      "|   ar.wiktionary.org|                262|             2335|               2597|           ar| wiktionary|\n",
      "| ar.m.wiktionary.org|                448|             1577|               2025|           ar| wiktionary|\n",
      "|ar.m.wikiversity.org|                389|              958|               1347|           ar|wikiversity|\n",
      "|  ar.m.wikibooks.org|                378|              855|               1233|           ar|  wikibooks|\n",
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "ar_site_visitors = wiki_uniq_df.filter(wiki_uniq_df.language_code == 'ar')\n",
    "\n",
    "# show the DataFrame\n",
    "ar_site_visitors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s find out which domain was visited the most. Filter the dataset to the rows for the language code \"ar\" and the columns domain and uniq_human_visitors. Save your result as a DataFrame named ar_visitors_slim and display it in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|              domain|uniq_human_visitors|\n",
      "+--------------------+-------------------+\n",
      "|  ar.m.wikipedia.org|            1644253|\n",
      "|    ar.wikipedia.org|             212695|\n",
      "| ar.m.wikisource.org|              56124|\n",
      "|   ar.wikisource.org|               2134|\n",
      "|  ar.m.wikiquote.org|                776|\n",
      "|   ar.wiktionary.org|                262|\n",
      "| ar.m.wiktionary.org|                448|\n",
      "|ar.m.wikiversity.org|                389|\n",
      "|  ar.m.wikibooks.org|                378|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "ar_visitors_slim = ar_site_visitors.select(['domain','uniq_human_visitors'])\n",
    "\n",
    "# show the DataFrame\n",
    "ar_visitors_slim.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the sum of all uniq_human_visitors grouped by site_type and ordered from highest to lowest by sum of visitors. Save your result as a DataFrame named top_visitors_site_type and display it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+\n",
      "|  site_type|sum(uniq_human_visitors)|\n",
      "+-----------+------------------------+\n",
      "|  wikipedia|               116527479|\n",
      "| wiktionary|                  892193|\n",
      "|  wikimedia|                  312995|\n",
      "| wikisource|                  172179|\n",
      "|   wikidata|                   69744|\n",
      "|  wikibooks|                   54680|\n",
      "|  wikiquote|                   38048|\n",
      "| wikivoyage|                   14648|\n",
      "|       wiki|                   13067|\n",
      "|wikiversity|                   12548|\n",
      "|   wikinews|                    5578|\n",
      "|   wikitech|                     751|\n",
      "+-----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "top_visitors_site_type = wiki_uniq_df\\\n",
    "                                    .select(['site_type','uniq_human_visitors'])\\\n",
    "                                    .groupBy('site_type')\\\n",
    "                                    .sum()\\\n",
    "                                    .orderBy('sum(uniq_human_visitors)', ascending=False)\n",
    "\n",
    "# show the DataFrame\n",
    "top_visitors_site_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying PySpark with SQL\n",
    "\n",
    "PySpark DataFrame’s query methods are an improvement on performing analysis directly on RDDs. However, working with DataFrame methods still requires some practice, and the code can become quite verbose. Luckily, we can analyze data in Spark with standard SQL through the SparkSession.sql() method. This exercise will closely mirror the previous one, and we’ll answer the same questions from that exercise using standard SQL.\n",
    "\n",
    "Before querying a DataFrame with SQL in Spark, it must be saved to the SparkSession’s catalog. The following code saves the DataFrame as a local temporary view in memory. As long as the current SparkSession is active, we can use SparkSession.sql() to query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df.createOrReplaceTempView('hourly_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the three sections of SQL below performs the same function as the DataFrame query methods described in the previous exercise. With the query below, we can filter our data to pages from a specific Wikipedia language_code (e.g., \"kw.m\") using a WHERE clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * FROM hourly_counts WHERE language_code = 'kw.m'\"\"\"\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query below, we display all pages with \"kw.m\" as their language_code ordered by the hourly_count using an ORDER BY clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT language_code, article_title, hourly_count\n",
    "    FROM hourly_counts\n",
    "    WHERE language_code = 'kw.m'\n",
    "    ORDER BY hourly_count DESC\"\"\"\n",
    "\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we select the sum of hourly_count by language_code over the entire DataFrame using a SQL statement with GROUP BY, SUM, and ORDER BY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT language_code, SUM(hourly_count) as sum_hourly_count\n",
    "    FROM hourly_counts\n",
    "    GROUP BY language_code\n",
    "    ORDER BY sum_hourly_count DESC\"\"\"\n",
    "\n",
    "spark.sql(query).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although querying data with SQL and DataFrame methods may look quite different, behind the scenes, Spark SQL translates everything to the same internal code. This means that as a developer, you can focus more on writing code for analysis in your preferred style rather than low-level execution details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame wiki_uniq_df has been saved to the current SparkSession as a temporary view named uniq_visitors_march.\n",
    "\n",
    "Write the SQL query that filters the dataset to the rows where language_code is 'ar'. Save your SQL string as a variable named ar_site_visitors_qry. Run that query and display the resulting DataFrame in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")\n",
    "\n",
    "# Create a temporary view with the DataFrame\n",
    "wiki_uniq_df\\\n",
    "    .createOrReplaceTempView('uniq_visitors_march')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "|domain              |uniq_human_visitors|uniq_bot_visitors|total_visitor_count|language_code|site_type  |\n",
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "|ar.m.wikipedia.org  |1644253            |750620           |2394873            |ar           |wikipedia  |\n",
      "|ar.wikipedia.org    |212695             |97700            |310395             |ar           |wikipedia  |\n",
      "|ar.m.wikisource.org |56124              |52885            |109009             |ar           |wikisource |\n",
      "|ar.wikisource.org   |2134               |4355             |6489               |ar           |wikisource |\n",
      "|ar.m.wikiquote.org  |776                |3511             |4287               |ar           |wikiquote  |\n",
      "|ar.wiktionary.org   |262                |2335             |2597               |ar           |wiktionary |\n",
      "|ar.m.wiktionary.org |448                |1577             |2025               |ar           |wiktionary |\n",
      "|ar.m.wikiversity.org|389                |958              |1347               |ar           |wikiversity|\n",
      "|ar.m.wikibooks.org  |378                |855              |1233               |ar           |wikibooks  |\n",
      "+--------------------+-------------------+-----------------+-------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "ar_site_visitors_qry = \"\"\" \n",
    "Select\n",
    "    *\n",
    "From\n",
    "    uniq_visitors_march\n",
    "Where \n",
    "    language_code='ar'\n",
    "\"\"\"\n",
    "\n",
    "# show the DataFrame\n",
    "spark.sql(ar_site_visitors_qry).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset to the rows where language_code is 'ar' and the columns are domain and uniq_human_visitors. Save your SQL string as a variable named ar_site_visitors_slim_qry. Run that query and display the resulting DataFrame in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|domain              |uniq_human_visitors|\n",
      "+--------------------+-------------------+\n",
      "|ar.m.wikipedia.org  |1644253            |\n",
      "|ar.wikipedia.org    |212695             |\n",
      "|ar.m.wikisource.org |56124              |\n",
      "|ar.wikisource.org   |2134               |\n",
      "|ar.m.wikiquote.org  |776                |\n",
      "|ar.wiktionary.org   |262                |\n",
      "|ar.m.wiktionary.org |448                |\n",
      "|ar.m.wikiversity.org|389                |\n",
      "|ar.m.wikibooks.org  |378                |\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "ar_site_visitors_slim_qry = \"\"\" \n",
    "Select\n",
    "    domain\n",
    "    ,uniq_human_visitors\n",
    "From\n",
    "    uniq_visitors_march\n",
    "Where \n",
    "    language_code='ar'\n",
    "\"\"\"\n",
    "# show the DataFrame\n",
    "spark.sql(ar_site_visitors_slim_qry).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the sum of all uniq_human_visitors grouped by site_type and ordered from highest to lowest by sum of visitors. Save your SQL string as a variable named site_top_type_qry. Run that query and display the resulting DataFrame in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+\n",
      "|site_type  |sum(uniq_human_visitors)|\n",
      "+-----------+------------------------+\n",
      "|wikipedia  |116527479               |\n",
      "|wiktionary |892193                  |\n",
      "|wikimedia  |312995                  |\n",
      "|wikisource |172179                  |\n",
      "|wikidata   |69744                   |\n",
      "|wikibooks  |54680                   |\n",
      "|wikiquote  |38048                   |\n",
      "|wikivoyage |14648                   |\n",
      "|wiki       |13067                   |\n",
      "|wikiversity|12548                   |\n",
      "|wikinews   |5578                    |\n",
      "|wikitech   |751                     |\n",
      "+-----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "site_top_type_qry = \"\"\" \n",
    "Select\n",
    "    site_type\n",
    "    ,sum(uniq_human_visitors)\n",
    "From\n",
    "    uniq_visitors_march\n",
    "Group by \n",
    "    site_type\n",
    "Order by \n",
    "    sum(uniq_human_visitors) desc\n",
    "\"\"\"\n",
    "\n",
    "# show the DataFrame\n",
    "spark.sql(site_top_type_qry).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving PySpark DataFrames\n",
    "\n",
    "Once you’ve done some analysis, the next step is often saving the transformed data back to disk for others to use. In this final topic, we’re going to cover how to efficiently save PySpark DataFrames.\n",
    "\n",
    "Similar to the SparkSession.read() method, Spark offers a SparkSession.write() method. Let’s perform a slight modification to our original Wikipedia views dataset and save it to disk. This code just uses .select() to select all columns except the monthly_count column (recall that earlier we discovered this column only contains zeros).\n",
    "\n",
    "Because Spark runs all operations in parallel, it’s typical to write DataFrames to a directory of files rather than a single CSV file. In the example below, Spark will split the underlying dataset and write multiple CSV files to cleaned/csv/views_2022_01_01_000000/. We can also use the mode argument of the .csv() method to overwrite any existing data in the target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrly_views_df\\\n",
    "    .select(['language_code', 'article_title', 'hourly_count'])\\\n",
    "    .write.csv('cleaned/csv/views_2022_01_01_000000/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SparkSession.read(), we can read the data from disk and confirm that it looks the same as the DataFrame we saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame back from disk\n",
    "hrly_views_df_restored = spark.read\\\n",
    "    .csv('cleaned/csv/views_2022_01_01_000000/')\n",
    "hrly_views_df_restored.printSchema()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Close, but not quite! It looks like this file didn’t retain information about column headers or datatypes. Unfortunately, there’s no way for a CSV to retain information about its format. Each time we read it, we’ll need to tell Spark exactly how it must be processed.\n",
    "\n",
    "Luckily, there is a file format called “Parquet” that’s specially designed for big data and solves this problem among many others. Parquet offers efficient data compression, is faster to perform analysis on than CSV, and preserves information about a dataset’s schema. Let’s try saving and re-reading this file to and from Parquet instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet\n",
    "hrly_views_slim_df\n",
    "    .write.parquet('cleaned/parquet/views_2022_01_01_000000/', mode=\"overwrite\")\n",
    "\n",
    "# Read Parquet as DataFrame\n",
    "hrly_views_df_restored = spark.read\\\n",
    "    .parquet('cleaned/parquet/views_2022_01_01_000000/')\n",
    "\n",
    "# Check DataFrame's schema\n",
    "hrly_views_df_restored.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now anyone who wants to query this data can do so with the much more efficient Parquet data format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the provided code to create a new DataFrame named uniq_human_visitors_df from wiki_uniq_df with only two columns: domain and uniq_human_visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|            domain|uniq_human_visitors|\n",
      "+------------------+-------------------+\n",
      "|en.m.wikipedia.org|           33261399|\n",
      "|  en.wikipedia.org|           17009339|\n",
      "|es.m.wikipedia.org|            5668575|\n",
      "|ru.m.wikipedia.org|            5816762|\n",
      "|ja.m.wikipedia.org|            5396108|\n",
      "|de.m.wikipedia.org|            4439596|\n",
      "|fr.m.wikipedia.org|            3798528|\n",
      "|  ru.wikipedia.org|            2852296|\n",
      "|  es.wikipedia.org|            2460489|\n",
      "|it.m.wikipedia.org|            2806943|\n",
      "|  de.wikipedia.org|            2252670|\n",
      "|  ja.wikipedia.org|            2128471|\n",
      "|  fr.wikipedia.org|            1839196|\n",
      "|zh.m.wikipedia.org|            2123391|\n",
      "|ar.m.wikipedia.org|            1644253|\n",
      "|pt.m.wikipedia.org|            1471752|\n",
      "|pl.m.wikipedia.org|            1410339|\n",
      "|fa.m.wikipedia.org|            1194940|\n",
      "|  zh.wikipedia.org|            1088755|\n",
      "|tr.m.wikipedia.org|             908573|\n",
      "+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only domain and uniq_human visitors\n",
    "uniq_human_visitors_df = wiki_uniq_df\\\n",
    "    .select('domain', 'uniq_human_visitors')\n",
    "\n",
    "# show the new DataFrame\n",
    "uniq_human_visitors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve modified a DataFrame, let’s persist the results as CSV files. Save uniq_human_visitors_df to a local directory, ./results/csv/uniq_human_visitors/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43muniq_human_visitors_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/csv/uniq_human_visitors/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "uniq_human_visitors_df.write.csv('./results/csv/uniq_human_visitors/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is PySpark’s preferred data format and saving our results in this format could expedite future analysis. Let’s persist the results as parquet files, too. Save uniq_human_visitors_df to a local directory, ./results/pq/uniq_human_visitors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o390.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43muniq_human_visitors_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/pq/uniq_human_visitors/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Documentos\\Codecademy\\BigDataPySpark\\BigDataPySpark.env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o390.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "uniq_human_visitors_df.write.parquet('./results/pq/uniq_human_visitors/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "The Spark ecosystem can be quite expansive, but the skills you’ve gained from this lesson should help you as you begin to branch out and run your own analyses. In this lesson you’ve learned:\n",
    "\n",
    "* How to construct Spark DataFrames from raw data in Python and Spark RDDs.\n",
    "\n",
    "* How to read and write data from disk into Spark DataFrames, including an introduction to file formats optimized for big-data workloads.\n",
    "\n",
    "* How to perform data exploration and cleaning on distributed data.\n",
    "\n",
    "* How the PySpark SQL API can allow you to perform analysis on distributed data more easily than working directly with RDDs by using DataFrames.\n",
    "\n",
    "* How to use the PySpark SQL API to query your datasets with standard SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataPySpark.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
